{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Netflix Movies and TV Shows Clustering\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 - Vaishnavi Sahu**\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project focuses on performing a comprehensive Exploratory Data Analysis (EDA) of Netflix’s catalog of movies and TV shows to uncover meaningful insights about the platform’s content strategy, audience preferences, and business direction.\n",
        "The dataset used contains 7,787 records and 12 columns, providing information such as title, type (Movie or TV Show), director, cast, country, date added, release year, rating, duration, genres (listed_in), and description. The data was sourced from Flixable, a third-party Netflix search engine, and represents titles available on Netflix up to 2019.\n",
        "\n",
        "In addition to EDA, this project serves as the foundation for a machine learning (ML) task using K-Means clustering, where the goal is to group similar titles based on features such as genre, rating, duration, and release period.\n",
        "The insights derived from EDA help define relevant features, guide preprocessing steps, and ensure the data is well-prepared for clustering analysis.\n",
        "\n",
        "1. Data Cleaning and Preprocessing\n",
        "The initial step involved inspecting and cleaning the dataset. Columns such as director, cast, country, and rating contained missing values, which were replaced with \"Unknown\" to retain records without losing data.\n",
        "The date_added column was converted into a proper datetime format, and two new columns — year_added and month_added — were derived for trend analysis.\n",
        "The duration column was further split into duration_minutes (for movies) and seasons (for TV shows) to enable quantitative analysis across both types of content.\n",
        "The dataset was found to be free of duplicates, with minimal data inconsistencies, making it ready for both visual exploration and machine learning applications.\n",
        "\n",
        "2. Exploratory Data Analysis (EDA)\n",
        "The EDA process involved generating a series of visualizations to interpret Netflix’s content trends:\n",
        "\n",
        "Content Distribution: Movies dominate the catalog with around 70% share, while TV Shows make up the remaining 30%. However, the number of TV Shows has increased significantly after 2015, showing Netflix’s growing investment in serialized content.\n",
        "Yearly and Monthly Trends: Most titles were added to Netflix between 2016–2019, indicating aggressive content expansion during that period. Content additions are evenly spread across months, ensuring consistent user engagement throughout the year.\n",
        "Regional Contributions: The United States leads with the highest number of titles, followed by India, the United Kingdom, and Japan. A deeper comparison between India and the US showed that while the US dominates in volume, India has shown a steep growth trajectory since 2016 — reflecting Netflix’s focus on international markets and localization.\n",
        "Ratings: Ratings analysis revealed that most titles are rated TV-MA and TV-14, implying a heavy focus on adult and teenage audiences. Family-friendly content such as PG or TV-Y is relatively limited, representing an opportunity for expansion.\n",
        "Duration and Seasons: Most movies are between 80–120 minutes, and most TV shows have 1–2 seasons, aligning with Netflix’s binge-watch culture.\n",
        "Genres: Popular genres include International Movies, Dramas, Comedies, and Documentaries, showing a preference for storytelling variety and global appeal.\n",
        "Directors and Cast: A few directors, like Raúl Campos, Jan Suter, and Marcus Raboy, contribute multiple titles, often in stand-up or documentary categories, highlighting Netflix’s collaboration with consistent creators.\n",
        "\n",
        "3. Numerical Analysis\n",
        "A correlation heatmap and pair plot were used to explore relationships between numerical variables such as release year, year added, duration, and seasons.\n",
        "The results showed weak correlations, implying Netflix’s diverse catalog does not follow rigid patterns — content length, release year, and addition time are largely independent.\n",
        "This diversity demonstrates Netflix’s strategy of providing a balanced range of content to appeal to varied audiences, and also ensures feature independence — an ideal property for unsupervised learning models like K-Means clustering.\n",
        "\n",
        "4. Key Insights and Business Recommendations\n",
        "From the analysis, Netflix appears to be successfully pursuing a data-driven, globally diverse, and modernized content strategy. The findings suggest:\n",
        "\n",
        "Continue expanding international content, especially regional originals in fast-growing markets like India.\n",
        "Invest in family and educational content to attract more household subscribers.\n",
        "Balance between shorter mini-series and longer-running shows to engage both casual and loyal viewers.\n",
        "Maintain consistent monthly content releases to sustain engagement.\n",
        "Diversify collaborations with new directors and storytellers for fresh creative perspectives.\n",
        "\n",
        "5. Conclusion\n",
        "Overall, this project demonstrates that Netflix’s success lies in its content diversity, global expansion, and responsiveness to audience behavior.\n",
        "The EDA results provide strong evidence of Netflix’s shift from being a US-centric platform to a global entertainment powerhouse.\n",
        "\n",
        "The insights gained here also lay the groundwork for the machine learning phase, where K-Means clustering will be applied to group similar content based on patterns in duration, rating, genre, and release information.\n",
        "This clustering will help Netflix identify content similarities, design recommendation systems, and optimize catalog management for different audience segments.\n",
        "\n",
        "Netflix’s data-driven approach, supported by both EDA insights and ML techniques, ensures it remains a leader in the competitive streaming industry — continuously evolving to meet the needs of a global audience."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Netflix, one of the world’s largest OTT platforms, offers an extensive and diverse catalog of Movies and TV Shows across multiple genres, countries, and languages. As the library continues to expand rapidly, understanding content distribution patterns, audience preferences, and regional trends has become a key business need.\n",
        "\n",
        "The main objective of this project is to analyze the Netflix dataset using Exploratory Data Analysis (EDA) to uncover meaningful insights about content type, growth over time, genres, and regional focus. Furthermore, by implementing a K-Means Clustering model, the project aims to group similar titles based on attributes like genre, duration, and release patterns.\n",
        "\n",
        "These insights will help Netflix enhance its recommendation system, identify content gaps and trends, and support data-driven decisions for global content strategy and audience engagement."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "# Core Data Manipulation and Analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Data Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# Set the visualization style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
        "\n",
        "# Machine Learning (Scikit-learn)\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import silhouette_score\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"All necessary libraries imported successfully!\")"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "df = pd.read_csv('/content/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(\"Dataset Shape:\", df.shape)"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "duplicate_count = df.duplicated().sum()\n",
        "print(\"Number of duplicate rows in the dataset:\", duplicate_count)"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"Missing values per column:\\n\", missing_values)\n",
        "\n",
        "# Total missing values in the dataset\n",
        "total_missing = df.isnull().sum().sum()\n",
        "print(\"\\nTotal missing values in the dataset:\", total_missing)"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
        "plt.title(\"Missing Values Heatmap\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "print(\"Columns in the dataset:\\n\", df.columns.tolist())"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "print(\"Unique values per column:\")\n",
        "for col in df.columns:\n",
        "    print(f\"{col}: {df[col].nunique()}\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "\n",
        "# 1️ Handle Duplicate Rows\n",
        "duplicate_count = df.duplicated().sum()\n",
        "print(\"Number of duplicate rows:\", duplicate_count)\n",
        "\n",
        "if duplicate_count > 0:\n",
        "    df = df.drop_duplicates()\n",
        "    print(\"Duplicates removed. New dataset shape:\", df.shape)\n",
        "else:\n",
        "    print(\"No duplicate rows found.\")\n",
        "\n",
        "# 2️ Handle Missing Values\n",
        "print(\"\\nMissing values per column before cleaning:\\n\", df.isnull().sum())\n",
        "\n",
        "for col in df.select_dtypes(include=np.number).columns:\n",
        "    df[col] = df[col].fillna(df[col].mean())\n",
        "\n",
        "for col in df.select_dtypes(include='object').columns:\n",
        "    df[col] = df[col].fillna(df[col].mode()[0])\n",
        "\n",
        "print(\"\\nMissing values per column after cleaning:\\n\", df.isnull().sum())\n",
        "\n",
        "# 3 Correct Data Types\n",
        "\n",
        "for col in df.select_dtypes(include='object').columns:\n",
        "    df[col] = df[col].astype('category')\n",
        "\n",
        "print(\"\\nUpdated Data Types:\\n\", df.dtypes)\n",
        "\n",
        "# 4️ Clean Text / Categorical Columns\n",
        "for col in df.select_dtypes(include='object').columns:\n",
        "    df[col] = df[col].str.strip().str.lower()\n",
        "print(\"\\nData Wrangling Completed. Dataset is clean and ready for analysis!\")\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(data=df, x='type', palette='viridis')\n",
        "plt.title(\"Distribution of Content Types\")\n",
        "plt.xlabel(\"Content Type\")\n",
        "plt.ylabel(\"Number of Titles\")\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To visualize how many Movies and TV Shows are available on Netflix.**"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reveals which type dominates the platform (e.g., more Movies than TV Shows).**"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helps Netflix balance its catalog between Movies and TV Shows.**"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure 'date_added' is in datetime format\n",
        "df['date_added'] = pd.to_datetime(df['date_added'], errors='coerce')\n",
        "\n",
        "# Extract year from 'date_added'\n",
        "df['year_added'] = df['date_added'].dt.year\n",
        "\n",
        "# Group by year and count titles\n",
        "titles_per_year = df['year_added'].value_counts().sort_index()\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(titles_per_year.index, titles_per_year.values, marker='o', linestyle='-', color='teal')\n",
        "plt.title(\"Number of Titles Added per Year on Netflix\")\n",
        "plt.xlabel(\"Year Added\")\n",
        "plt.ylabel(\"Number of Titles\")\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To observe how the volume of new content added to Netflix has changed over the years.**"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Highlights years when Netflix added the most titles or slowed down content additions.**"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helps Netflix analyze its content growth strategy and expansion trends over time.**"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.histplot(df['release_year'], bins=30, kde=False, color='royalblue')\n",
        "plt.title(\"Distribution of Content by Release Year\")\n",
        "plt.xlabel(\"Release Year\")\n",
        "plt.ylabel(\"Number of Titles\")\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To visualize how many titles were released in each year.**"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Shows whether Netflix hosts more classic or recent content. For instance, a spike in recent years means Netflix prefers newer releases.**"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helps in understanding content age distribution and licensing focus.**"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# Ensure date format and extract year\n",
        "df['date_added'] = pd.to_datetime(df['date_added'], errors='coerce')\n",
        "df['year_added'] = df['date_added'].dt.year\n",
        "\n",
        "# Group by year and type\n",
        "tv_movie_trend = df.groupby(['year_added', 'type']).size().unstack(fill_value=0)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10,5))\n",
        "tv_movie_trend.plot(kind='line', marker='o', figsize=(10,5))\n",
        "plt.title(\"TV Shows vs Movies Over the Years\")\n",
        "plt.xlabel(\"Year Added\")\n",
        "plt.ylabel(\"Number of Titles\")\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title='Type')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To compare the trend of TV Shows and Movies added over time on Netflix.**"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Shows whether Netflix’s focus shifted from Movies to TV Shows (e.g., steady growth in TV Shows post-2015).**"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helps Netflix make strategic decisions about future content investments and audience engagement.**"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 10 Countries by Number of Titles\n",
        "\n",
        "top_countries = df['country'].dropna().value_counts().head(10)\n",
        "\n",
        "# Create clean bar plot\n",
        "plt.figure(figsize=(10,6))\n",
        "bars = plt.barh(top_countries.index[::-1], top_countries.values[::-1], color='#00BFC4', edgecolor='black')\n",
        "\n",
        "# Title and labels\n",
        "plt.title(\"Top 10 Countries by Number of Titles on Netflix\", fontsize=14, fontweight='bold')\n",
        "plt.xlabel(\"Number of Titles\", fontsize=12)\n",
        "plt.ylabel(\"Country\", fontsize=12)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar in bars:\n",
        "    plt.text(bar.get_width() + 5, bar.get_y() + bar.get_height()/2,\n",
        "             f'{int(bar.get_width())}', va='center', fontsize=10)\n",
        "\n",
        "# Clean layout\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A horizontal bar chart clearly shows which countries contribute the most Netflix titles.**"
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Highlights that certain countries dominate Netflix’s content library.**"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helps identify strong regions for content partnerships and regional strategy.**"
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get top 10 countries\n",
        "top_countries = df['country'].dropna().value_counts().head(10).index\n",
        "\n",
        "# Group by country and type\n",
        "country_type = df[df['country'].isin(top_countries)].groupby(['country', 'type']).size().unstack(fill_value=0)\n",
        "\n",
        "# Create clean heatmap\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.heatmap(country_type, annot=True, fmt='d', cmap='YlGnBu', linewidths=0.5, cbar_kws={'label': 'Number of Titles'})\n",
        "\n",
        "plt.title(\"Content Type Distribution by Country\", fontsize=14, fontweight='bold', pad=15)\n",
        "plt.xlabel(\"Content Type\", fontsize=12)\n",
        "plt.ylabel(\"Country\", fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A heatmap provides a clean way to compare Movies vs TV Shows across countries.**"
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**You can instantly spot countries where one type dominates.**"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helps Netflix align its content offerings with audience demand in each country.**"
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get top 10 directors\n",
        "top_directors = df['director'].dropna().value_counts().head(10)\n",
        "\n",
        "# Simple vertical bar chart\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.bar(top_directors.index, top_directors.values, color='skyblue', edgecolor='black')\n",
        "\n",
        "plt.title(\"Top 10 Directors by Number of Titles\", fontsize=14, fontweight='bold')\n",
        "plt.xlabel(\"Director\", fontsize=12)\n",
        "plt.ylabel(\"Number of Titles\", fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To quickly see which directors have contributed the most content to Netflix.**"
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Identifies prolific directors whose works dominate the platform.**"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helps Netflix in content partnerships, marketing, and promotions.**"
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'cast' column to string to avoid categorical issues\n",
        "df['cast'] = df['cast'].astype(str)\n",
        "\n",
        "# Explode multiple actors per title\n",
        "actors_series = df['cast'].str.split(',').explode().str.strip()\n",
        "\n",
        "# Get top 10 actors\n",
        "top_actors = actors_series.value_counts().head(10)\n",
        "\n",
        "# Simple bar chart (Versus style)\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.bar(top_actors.index, top_actors.values, color='coral', edgecolor='black')\n",
        "\n",
        "plt.title(\"Top 10 Actors by Number of Appearances\", fontsize=14, fontweight='bold')\n",
        "plt.xlabel(\"Actor\", fontsize=12)\n",
        "plt.ylabel(\"Number of Titles\", fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To compare the most frequent actors across Netflix content.**"
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Highlights which actors dominate the platform’s library.**"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helps in casting, promotions, and partnership decisions.**"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'rating' to string to avoid categorical issues\n",
        "df['rating'] = df['rating'].astype(str)\n",
        "\n",
        "# Replace empty or missing ratings with 'Not Rated'\n",
        "df['rating'] = df['rating'].replace(['', 'nan', 'NaN'], 'Not Rated')\n",
        "\n",
        "# Count ratings\n",
        "ratings_counts = df['rating'].value_counts()\n",
        "\n",
        "# Simple vertical bar chart\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.bar(ratings_counts.index, ratings_counts.values, color='mediumseagreen', edgecolor='black')\n",
        "\n",
        "plt.title(\"Ratings Distribution of Netflix Titles\", fontsize=14, fontweight='bold')\n",
        "plt.xlabel(\"Rating\", fontsize=12)\n",
        "plt.ylabel(\"Number of Titles\", fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To see how Netflix content is rated across different audiences.**"
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Identifies which ratings are most common (e.g., TV-MA, PG-13).**"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helps Netflix ensure content meets audience preferences and compliance requirements.**"
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing genres\n",
        "df['listed_in'] = df['listed_in'].astype(str)\n",
        "\n",
        "# Split multiple genres per title and explode\n",
        "genres_series = df['listed_in'].str.split(',').explode().str.strip()\n",
        "\n",
        "# Count top 10 genres\n",
        "top_genres = genres_series.value_counts().head(10)\n",
        "\n",
        "# Pie chart\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.pie(top_genres.values, labels=top_genres.index, autopct='%1.1f%%', startangle=140, colors=plt.cm.Paired.colors)\n",
        "plt.title(\"Top 10 Netflix Genres\", fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pie charts show the proportion of each genre in a visually simple way.**"
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quickly identifies the most common genres on Netflix (e.g., Drama, Comedy).**"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helps Netflix understand popular content categories for production and recommendation strategies.**"
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure duration is numeric\n",
        "import re\n",
        "\n",
        "def extract_duration(duration):\n",
        "    if pd.isna(duration):\n",
        "        return None\n",
        "    match = re.search(r'(\\d+)', str(duration))\n",
        "    return int(match.group(1)) if match else None\n",
        "\n",
        "df['duration_minutes'] = df['duration'].apply(extract_duration)\n",
        "\n",
        "# Split genres and explode once\n",
        "df['listed_in'] = df['listed_in'].astype(str)\n",
        "df_genres = df.assign(listed_in=df['listed_in'].str.split(',')).explode('listed_in')\n",
        "df_genres['listed_in'] = df_genres['listed_in'].str.strip()\n",
        "\n",
        "# Filter top 10 genres for clarity\n",
        "top_genres = df_genres['listed_in'].value_counts().head(10).index\n",
        "df_plot = df_genres[df_genres['listed_in'].isin(top_genres)]\n",
        "\n",
        "# Horizontal boxplot\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.boxplot(x='duration_minutes', y='listed_in', data=df_plot, palette='Set2')\n",
        "plt.title(\"Duration Distribution by Top 10 Genres\", fontsize=14, fontweight='bold')\n",
        "plt.xlabel(\"Duration (Minutes)\", fontsize=12)\n",
        "plt.ylabel(\"Genre\", fontsize=12)\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Boxplot shows the spread of content durations per genre.**"
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Identifies which genres have longer or shorter average durations.**"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helps Netflix tailor content length per genre to improve viewer engagement.**"
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Release Year vs Duration\n",
        "\n",
        "# Remove rows with missing release_year or duration\n",
        "df_scatter = df.dropna(subset=['release_year', 'duration_minutes'])\n",
        "\n",
        "# Scatter plot\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.scatter(df_scatter['release_year'], df_scatter['duration_minutes'], alpha=0.6, color='slateblue', edgecolors='black')\n",
        "\n",
        "plt.title(\"Release Year vs Duration of Netflix Titles\", fontsize=14, fontweight='bold')\n",
        "plt.xlabel(\"Release Year\", fontsize=12)\n",
        "plt.ylabel(\"Duration (Minutes)\", fontsize=12)\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To visualize how content duration has changed over the years.**"
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Identify trends like whether newer titles tend to be longer or shorter.**"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helps Netflix in planning content length according to viewer preferences over time.**"
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of Numeric Features\n",
        "# Select numeric columns\n",
        "numeric_cols = ['duration_minutes', 'release_year', 'year_added']\n",
        "\n",
        "# Drop missing values\n",
        "df_numeric = df[numeric_cols].dropna()\n",
        "\n",
        "# Plot histograms\n",
        "plt.figure(figsize=(12,4))\n",
        "\n",
        "for i, col in enumerate(numeric_cols):\n",
        "    plt.subplot(1, len(numeric_cols), i+1)\n",
        "    plt.hist(df_numeric[col], bins=20, color='skyblue', edgecolor='black')\n",
        "    plt.title(f\"{col} Distribution\")\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To visualize the distribution of key numeric features in Netflix data.**"
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helps identify typical durations, release years, and content addition years.**"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Guides understanding of content trends and production planning.**"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap\n",
        "# Select numeric columns\n",
        "numeric_cols = ['duration_minutes', 'release_year', 'year_added']\n",
        "\n",
        "# Drop missing values\n",
        "df_numeric = df[numeric_cols].dropna()\n",
        "\n",
        "# Compute correlation\n",
        "corr = df_numeric.corr()\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(corr, annot=True, cmap='Blues', linewidths=0.5, fmt=\".2f\")\n",
        "plt.title(\"Correlation Heatmap of Numeric Features\", fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To examine relationships between numeric variables like duration, release year, and year added.**"
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reveals if features are positively or negatively correlated (e.g., newer titles may have longer durations).**"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pairplot of Key Numeric Features\n",
        "# Select numeric columns\n",
        "numeric_cols = ['duration_minutes', 'release_year', 'year_added']\n",
        "\n",
        "# Drop rows with missing values\n",
        "df_numeric = df[numeric_cols].dropna()\n",
        "\n",
        "# Pairplot\n",
        "sns.pairplot(df_numeric)\n",
        "plt.suptitle(\"Pairplot of Key Numeric Features\", fontsize=14, fontweight='bold', y=1.02)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To visualize relationships and patterns between multiple numeric variables simultaneously.**"
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helps identify trends, clusters, or potential correlations between features.**"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Research Question: Is Netflix adding more TV shows than movies on average per year?\n",
        "\n",
        "**Null Hypothesis (H0)**:\n",
        "\n",
        "The average number of TV shows added per year is equal to the average number of movies added per year.\n",
        "\n",
        "H\n",
        "0\n",
        "\t: μ\n",
        "TV\n",
        "\t​ = μ\n",
        "Movie\n",
        "\n",
        "**Alternative Hypothesis (H1)**:\n",
        "\n",
        "The average number of TV shows added per year is greater than the average number of movies added per year.\n",
        "\n",
        "H1 ​: μTV ​> μMovie\n",
        "\t​\n"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by year and type\n",
        "tv_movie_trend = df.groupby(['year_added','type']).size().unstack(fill_value=0)\n",
        "\n",
        "# Separate TV and Movie counts per year\n",
        "tv_counts = tv_movie_trend['TV Show']\n",
        "movie_counts = tv_movie_trend['Movie']\n",
        "\n",
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import ttest_rel\n",
        "\n",
        "# Perform one-tailed paired t-test\n",
        "t_stat, p_value_two_tailed = ttest_rel(tv_counts, movie_counts)\n",
        "\n",
        "# For one-tailed test (TV > Movie)\n",
        "p_value_one_tailed = p_value_two_tailed / 2\n",
        "\n",
        "print(f\"T-statistic: {t_stat:.3f}\")\n",
        "print(f\"One-tailed P-value: {p_value_one_tailed:.3f}\")\n",
        "\n",
        "alpha = 0.05  # significance level\n",
        "\n",
        "if (t_stat > 0) and (p_value_one_tailed < alpha):\n",
        "    print(\"Reject H0: Netflix is adding more TV shows than movies on average per year.\")\n",
        "else:\n",
        "    print(\"Fail to reject H0: No significant evidence that TV shows are added more than movies per year.\")\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statistical Test Used\n",
        "\n",
        "Test: Paired t-test (one-tailed)\n",
        "\n",
        "Purpose: To compare the average number of TV shows added per year vs the average number of movies added per year.\n",
        "\n",
        "Output: T-statistic and one-tailed p-value."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why This Test Was Chosen\n",
        "\n",
        "Nature of the Data:\n",
        "\n",
        "The data consists of numeric counts per year for two categories: TV shows and movies.\n",
        "\n",
        "Paired Observations:\n",
        "\n",
        "Each year provides a paired observation: number of TV shows vs number of movies.\n",
        "\n",
        "Hence, a paired t-test is appropriate to account for year-to-year pairing.\n",
        "\n",
        "Direction of Hypothesis:\n",
        "\n",
        "The alternative hypothesis is directional (TV shows > movies), so we use a one-tailed test.\n",
        "\n",
        "Sample Size:\n",
        "\n",
        "There are enough years (data points) to satisfy t-test assumptions for approximate normality of differences."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothetical Statement 2\n",
        "\n",
        "Research Question: Is the proportion of Drama titles higher than non-Drama titles on Netflix?\n",
        "\n",
        "**Null Hypothesis (H0)**:\n",
        "\n",
        "The proportion of Drama titles is equal to the proportion of non-Drama titles.\n",
        "H0​:pDrama​ = pNon−Drama​\n",
        "\n",
        "**Alternative Hypothesis (H1):**\n",
        "\n",
        "The proportion of Drama titles is higher than the proportion of non-Drama titles.\n",
        "H1​:pDrama​ > pNon−Drama​"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "from statsmodels.stats.proportion import proportions_ztest\n",
        "\n",
        "# Handle missing genres\n",
        "df['listed_in'] = df['listed_in'].astype(str)\n",
        "\n",
        "# Create binary variable: Drama = 1, Non-Drama = 0\n",
        "df['is_drama'] = df['listed_in'].str.contains('Drama', case=False, na=False).astype(int)\n",
        "\n",
        "# Count of Drama titles\n",
        "count_drama = df['is_drama'].sum()\n",
        "\n",
        "# Total number of titles\n",
        "n_total = len(df['is_drama'])\n",
        "\n",
        "# Null hypothesis proportion (50% for equal proportion)\n",
        "p_null = 0.5\n",
        "\n",
        "# Perform one-sample proportion z-test\n",
        "stat, p_value_two_tailed = proportions_ztest(count_drama, n_total, value=p_null)\n",
        "\n",
        "# One-tailed test (Drama > Non-Drama)\n",
        "p_value_one_tailed = p_value_two_tailed / 2\n",
        "\n",
        "print(f\"Z-statistic: {stat:.3f}\")\n",
        "print(f\"One-tailed P-value: {p_value_one_tailed:.3f}\")\n",
        "\n",
        "# Conclusion\n",
        "alpha = 0.05\n",
        "if (stat > 0) and (p_value_one_tailed < alpha):\n",
        "    print(\"Reject H0: The proportion of Drama titles is significantly higher than non-Drama titles.\")\n",
        "else:\n",
        "    print(\"Fail to reject H0: No significant evidence that Drama titles are higher than non-Drama titles.\")\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Purpose: To test whether the proportion of “Drama” titles is significantly greater than the proportion of non-Drama titles.\n",
        "\n",
        "Test Type: One-sample z-test for proportions (from statsmodels.stats.proportion.proportions_ztest).\n",
        "Null Hypothesis (H0):\n",
        "𝑝\n",
        "𝐷\n",
        "𝑟\n",
        "𝑎\n",
        "𝑚\n",
        "𝑎\n",
        "=  0.5 (Drama proportion equals non-Drama proportion)\n",
        "\n",
        "Alternative Hypothesis (H1):\n",
        "𝑝\n",
        "𝐷\n",
        "𝑟\n",
        "𝑎\n",
        "𝑚\n",
        "𝑎 > 0.5 (Drama proportion is higher)\n",
        "\n",
        "Why z-test: Because we are comparing an observed proportion to a theoretical proportion under H0, and the sample size is large enough for the z-test approximation.\n",
        "\n",
        "P-value: The probability of observing a proportion at least as extreme as the one in the data if the null hypothesis were true.\n",
        "One-tailed adjustment: Since H1 is directional (Drama > Non-Drama), we divide the two-tailed p-value by 2."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose the one-sample proportion z-test for Hypothesis 2 for the following reasons:\n",
        "\n",
        "Nature of the Data:\n",
        "\n",
        "The variable of interest is categorical: whether a title is “Drama” (1) or not (0).\n",
        "\n",
        "We are analyzing proportions, not means.\n",
        "\n",
        "Research Question:\n",
        "\n",
        "We want to test if the proportion of Drama titles is significantly higher than non-Drama titles.\n",
        "\n",
        "This requires comparing an observed proportion to a hypothetical proportion (null proportion = 0.5).\n",
        "\n",
        "Sample Size:\n",
        "\n",
        "Netflix dataset contains thousands of titles, making the z-test appropriate due to the large-sample approximation of the binomial distribution.\n",
        "\n",
        "Test Direction:\n",
        "\n",
        "Our alternative hypothesis is directional (Drama > Non-Drama), so a one-tailed z-test is suitable."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Research Question: Do Netflix titles released in 2015 or later have a higher average duration than titles released before 2015?\n",
        "\n",
        "**Null Hypothesis (H0):**\n",
        "\n",
        "The average duration of titles released in 2015 or later is equal to the average duration of titles released before 2015.\n",
        "H0​ : μ2015 + ​= μ < 2015​\n",
        "\n",
        "**Alternative Hypothesis (H1):**\n",
        "\n",
        "The average duration of titles released in 2015 or later is greater than the average duration of titles released before 2015.\n",
        "H1 ​: μ2015 + ​> μ < 2015​"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Ensure duration is numeric (already have 'duration_minutes')\n",
        "df_duration = df.dropna(subset=['duration_minutes', 'release_year'])\n",
        "\n",
        "# Create two groups\n",
        "group_before_2015 = df_duration[df_duration['release_year'] < 2015]['duration_minutes']\n",
        "group_2015_plus = df_duration[df_duration['release_year'] >= 2015]['duration_minutes']\n",
        "\n",
        "# Perform independent two-sample t-test\n",
        "t_stat, p_value_two_tailed = ttest_ind(group_2015_plus, group_before_2015, equal_var=False)  # Welch's t-test\n",
        "\n",
        "# For one-tailed test (2015+ > before 2015)\n",
        "p_value_one_tailed = p_value_two_tailed / 2\n",
        "\n",
        "print(f\"T-statistic: {t_stat:.3f}\")\n",
        "print(f\"One-tailed P-value: {p_value_one_tailed:.3f}\")\n",
        "\n",
        "# Conclusion\n",
        "alpha = 0.05\n",
        "if (t_stat > 0) and (p_value_one_tailed < alpha):\n",
        "    print(\"Reject H0: Titles released in 2015 or later have significantly higher average duration.\")\n",
        "else:\n",
        "    print(\"Fail to reject H0: No significant evidence that newer titles have longer duration.\")\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statistical Test Used\n",
        "\n",
        "Test: Independent two-sample t-test (Welch’s t-test)\n",
        "\n",
        "Purpose: To compare the average duration of two independent groups:\n",
        "\n",
        "Titles released before 2015\n",
        "\n",
        "Titles released in 2015 or later\n",
        "\n",
        "Output: T-statistic and one-tailed p-value"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reason for Choosing This Test\n",
        "\n",
        "Nature of the Data:\n",
        "\n",
        "The variable of interest (duration_minutes) is continuous numeric.\n",
        "\n",
        "We are comparing the means between two independent groups.\n",
        "\n",
        "Two Independent Groups:\n",
        "\n",
        "Group 1 = titles before 2015\n",
        "\n",
        "Group 2 = titles from 2015 onwards\n",
        "\n",
        "These groups are mutually exclusive, so an independent t-test is appropriate.\n",
        "\n",
        "Variance Consideration:\n",
        "\n",
        "The two groups may have unequal variances, so we use Welch’s t-test, which doesn’t assume equal variance.\n",
        "\n",
        "Direction of Hypothesis:\n",
        "\n",
        "Our alternative hypothesis is directional: newer titles have higher average duration.\n",
        "\n",
        "Hence, we use a one-tailed test.\n",
        "\n",
        "Large Enough Sample Size:\n",
        "\n",
        "Netflix dataset has many titles, so the t-test approximation is valid."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "# Check missing values in each column\n",
        "missing_values = df.isnull().sum()\n",
        "missing_percentage = (missing_values / len(df)) * 100\n",
        "\n",
        "missing_summary = pd.DataFrame({\n",
        "    'Missing Count': missing_values,\n",
        "    'Missing Percentage': missing_percentage\n",
        "})\n",
        "\n",
        "missing_summary"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop columns with more than 50% missing values\n",
        "df = df.loc[:, df.isnull().mean() < 0.5]"
      ],
      "metadata": {
        "id": "bcZ91lPmKgv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows where 'release_year' or 'type' is missing\n",
        "df = df.dropna(subset=['release_year', 'type'])\n"
      ],
      "metadata": {
        "id": "tGamFNQ4Klf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill categorical columns with 'Unknown'\n",
        "categorical_cols = df.select_dtypes(include='object').columns\n",
        "df[categorical_cols] = df[categorical_cols].fillna('Unknown')\n",
        "\n",
        "# Fill numeric columns with median\n",
        "numeric_cols = df.select_dtypes(include='number').columns\n",
        "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n"
      ],
      "metadata": {
        "id": "KQW8XDplKo5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check again\n",
        "df.isnull().sum()\n"
      ],
      "metadata": {
        "id": "azDzFc7TKs5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Missing Value Imputation Techniques Used and Rationale\n",
        "\n",
        "Dropping Columns with Too Many Missing Values\n",
        "\n",
        "Technique: Removed columns where more than 50% of the data was missing.\n",
        "\n",
        "Reason: Columns with excessive missing values provide little useful information and can introduce bias if imputed. Dropping them keeps the dataset clean and reliable.\n",
        "\n",
        "Dropping Rows with Critical Missing Values\n",
        "\n",
        "Technique: Dropped rows where critical columns like release_year or type were missing.\n",
        "\n",
        "Reason: These columns are essential for analysis and modeling. Imputing them could misrepresent key attributes like content type or release year, affecting analysis accuracy.\n",
        "\n",
        "Filling Missing Values in Categorical Columns\n",
        "\n",
        "Technique: Replaced missing categorical values (e.g., country, rating, cast) with 'Unknown'.\n",
        "\n",
        "Reason: Keeps categorical data consistent and allows inclusion in analysis without biasing frequency counts or charts.\n",
        "\n",
        "Filling Missing Values in Numeric Columns\n",
        "\n",
        "Technique: Replaced missing numeric values (e.g., duration_minutes) with the median of the column.\n",
        "\n",
        "Reason: Median is robust to outliers and preserves the central tendency, avoiding distortion of distributions compared to using mean"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "import seaborn as sns\n",
        "\n",
        "# Visual detection with boxplot\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.boxplot(df['duration_minutes'])\n",
        "plt.title(\"Boxplot of Duration (Minutes)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect outliers using IQR\n",
        "Q1 = df['duration_minutes'].quantile(0.25)\n",
        "Q3 = df['duration_minutes'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Define bounds\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "# Identify outliers\n",
        "outliers = df[(df['duration_minutes'] < lower_bound) | (df['duration_minutes'] > upper_bound)]\n",
        "print(f\"Number of outliers: {len(outliers)}\")\n"
      ],
      "metadata": {
        "id": "jiGM8-gxLWaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cap outliers to upper and lower bounds\n",
        "df['duration_minutes'] = df['duration_minutes'].apply(\n",
        "    lambda x: upper_bound if x > upper_bound else (lower_bound if x < lower_bound else x)\n",
        ")\n"
      ],
      "metadata": {
        "id": "fumQZUJhLWTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Capping (Winsorization):\n",
        "\n",
        "Used to limit extreme values within the IQR bounds.\n",
        "\n",
        "Maintains dataset size and reduces skew caused by extreme durations.\n",
        "\n",
        "Optional Removal:\n",
        "\n",
        "Could remove extreme outlier rows if they distort analysis.\n",
        "\n",
        "Optional Log Transformation:\n",
        "\n",
        "Used if the distribution is highly skewed for statistical modeling purposes."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "# Identify categorical columns\n",
        "categorical_cols = df.select_dtypes(include='object').columns\n",
        "categorical_cols\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Example: 'type' column\n",
        "le = LabelEncoder()\n",
        "df['type_encoded'] = le.fit_transform(df['type'])\n"
      ],
      "metadata": {
        "id": "NDwi2SR0MNKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: 'rating' column\n",
        "df = pd.get_dummies(df, columns=['rating'], prefix='rating', drop_first=True)\n"
      ],
      "metadata": {
        "id": "_t7j5I1VMNBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-label encoding for 'listed_in' (genres)\n",
        "# Split genres into multiple columns\n",
        "genres = df['listed_in'].str.get_dummies(sep=',')\n",
        "df = pd.concat([df, genres], axis=1)\n"
      ],
      "metadata": {
        "id": "RuyyqVOVMM3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Categorical Encoding Techniques Used and Rationale\n",
        "\n",
        "Label Encoding\n",
        "\n",
        "Columns Used: type (Movie / TV Show)\n",
        "\n",
        "Technique: Assign numeric values to each category (e.g., Movie = 0, TV Show = 1)\n",
        "\n",
        "Reason:\n",
        "\n",
        "Suitable for columns with small number of categories.\n",
        "\n",
        "Converts categorical data into numeric form for machine learning algorithms.\n",
        "\n",
        "Preserves simplicity without creating unnecessary extra columns.\n",
        "\n",
        "One-Hot Encoding\n",
        "\n",
        "Columns Used: rating, country (optional)\n",
        "\n",
        "Technique: Create binary columns for each category (1 if present, 0 if not)\n",
        "\n",
        "Reason:\n",
        "\n",
        "Used for nominal categorical variables where no ordinal relationship exists.\n",
        "\n",
        "Avoids implying order between categories, which can mislead models.\n",
        "\n",
        "Drop one column to avoid multicollinearity (dummy variable trap).\n",
        "\n",
        "Multi-Hot Encoding / Multi-Label Encoding\n",
        "\n",
        "Columns Used: listed_in (genres)\n",
        "\n",
        "Technique: Split multi-category entries and create a binary column for each genre\n",
        "\n",
        "Reason:\n",
        "\n",
        "Titles can belong to multiple genres; simple label or one-hot encoding cannot capture this.\n",
        "\n",
        "Enables multi-label analysis and machine learning models to understand genre combinations."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "contraction_dict = {\n",
        "    \"don't\": \"do not\",\n",
        "    \"can't\": \"cannot\",\n",
        "    \"it's\": \"it is\",\n",
        "    \"i'm\": \"i am\",\n",
        "    \"you're\": \"you are\",\n",
        "    # Add more as needed\n",
        "}\n",
        "\n",
        "def expand_contractions(text):\n",
        "    for key, value in contraction_dict.items():\n",
        "        text = text.replace(key, value)\n",
        "    return text\n",
        "\n",
        "df['description_clean'] = df['description'].astype(str).apply(expand_contractions)\n",
        "df['title_clean'] = df['title'].astype(str).apply(expand_contractions)\n"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "# Lowercase 'description'\n",
        "df['description_clean'] = df['description_clean'].astype(str).str.lower()\n",
        "\n",
        "# Lowercase 'title'\n",
        "df['title_clean'] = df['title_clean'].astype(str).str.lower()"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "import re\n",
        "import string\n",
        "\n",
        "# Remove punctuation from 'description'\n",
        "df['description_clean'] = df['description_clean'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x))\n",
        "\n",
        "# Remove punctuation from 'title'\n",
        "df['title_clean'] = df['title_clean'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x))\n"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "# Remove URLs from 'description'\n",
        "df['description_clean'] = df['description_clean'].apply(lambda x: re.sub(r'http\\S+|www\\S+|https\\S+', '', x, flags=re.MULTILINE))\n",
        "\n",
        "# Remove URLs from 'title'\n",
        "df['title_clean'] = df['title_clean'].apply(lambda x: re.sub(r'http\\S+|www\\S+|https\\S+', '', x, flags=re.MULTILINE))\n",
        "\n",
        "# Remove words containing numbers (like \"4K\", \"2023\")\n",
        "df['description_clean'] = df['description_clean'].apply(lambda x: ' '.join([word for word in x.split() if not any(char.isdigit() for char in word)]))\n",
        "\n",
        "df['title_clean'] = df['title_clean'].apply(lambda x: ' '.join([word for word in x.split() if not any(char.isdigit() for char in word)]))\n"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stopwords from 'description'\n",
        "df['description_clean'] = df['description_clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
        "\n",
        "# Remove stopwords from 'title'\n",
        "df['title_clean'] = df['title_clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "# Remove leading, trailing, and multiple spaces from 'description'\n",
        "df['description_clean'] = df['description_clean'].apply(lambda x: ' '.join(x.split()))\n",
        "\n",
        "# Remove leading, trailing, and multiple spaces from 'title'\n",
        "df['title_clean'] = df['title_clean'].apply(lambda x: ' '.join(x.split()))\n"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text\n",
        "# Define replacement dictionary for common phrases\n",
        "replacements = {\n",
        "    \"highly rated\": \"popular\",\n",
        "    \"based on true story\": \"true story\",\n",
        "    \"award winning\": \"famous\",\n",
        "    \"family friendly\": \"kids friendly\"\n",
        "}\n",
        "\n",
        "def rephrase_text(text):\n",
        "    for key, value in replacements.items():\n",
        "        text = text.replace(key, value)\n",
        "    return text\n",
        "\n",
        "# Apply to description\n",
        "df['description_clean'] = df['description_clean'].apply(rephrase_text)\n",
        "\n",
        "# Apply to title (if needed)\n",
        "df['title_clean'] = df['title_clean'].apply(rephrase_text)\n"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "# Simple whitespace-based tokenization\n",
        "df['description_tokens'] = df['description_clean'].apply(lambda x: str(x).split())\n",
        "df['title_tokens'] = df['title_clean'].apply(lambda x: str(x).split())\n"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize 'description' tokens\n",
        "df['description_tokens_lemmatized'] = df['description_tokens'].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])\n",
        "\n",
        "# Lemmatize 'title' tokens\n",
        "df['title_tokens_lemmatized'] = df['title_tokens'].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])\n"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Reduce Vocabulary Size: Converts variations of words to a single base form, improving consistency.\n",
        "\n",
        "2. Preserve Meaning: Unlike stemming, lemmatization ensures words remain meaningful.\n",
        "\n",
        "3. Improves NLP Models: Helps in text clustering, TF-IDF, wordclouds, and classification, as similar words are treated as one."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Use TF-IDF on 'description_clean'\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
        "\n",
        "# Fit and transform\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['description_clean'])\n",
        "\n",
        "print(tfidf_matrix.shape)\n",
        "# Output: (number_of_titles, 5000)\n"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF Vectorization (Term Frequency–Inverse Document Frequency)\n",
        "\n",
        "Why TF-IDF Was Used:\n",
        "\n",
        "Captures Word Importance:\n",
        "\n",
        "Unlike simple word counts (Bag-of-Words), TF-IDF emphasizes words that are unique or important in a document while down-weighting very common words.\n",
        "\n",
        "Example: In Netflix descriptions, “movie” appears everywhere and is less informative, whereas “thriller” or “space” carries more meaning.\n",
        "\n",
        "Reduces Noise:\n",
        "\n",
        "Common stopwords like “the”, “and”, “is” are ignored automatically.\n",
        "\n",
        "Focuses on meaningful keywords that contribute to clustering or content similarity.\n",
        "\n",
        "Better for NLP Tasks:\n",
        "\n",
        "Useful for clustering similar content, recommendation systems, or keyword extraction.\n",
        "\n",
        "Produces a sparse numeric matrix that can be used directly in ML algorithms.\n",
        "\n",
        "Flexible & Efficient:\n",
        "\n",
        "Can limit features with max_features to handle large datasets.\n",
        "\n",
        "Works well with textual datasets like Netflix titles and descriptions."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "# Ensure 'date_added' is datetime\n",
        "df['date_added'] = pd.to_datetime(df['date_added'], errors='coerce')\n",
        "\n",
        "# Extract year and month\n",
        "df['year_added'] = df['date_added'].dt.year\n",
        "df['month_added'] = df['date_added'].dt.month\n",
        "# Length of title\n",
        "df['title_length'] = df['title'].astype(str).apply(len)\n",
        "\n",
        "# Length of description\n",
        "df['description_length'] = df['description'].astype(str).apply(len)\n",
        "\n",
        "\n",
        "df['num_genres'] = df['listed_in'].astype(str).apply(lambda x: len(x.split(',')))\n",
        "df['num_actors'] = df['cast'].astype(str).apply(lambda x: len(x.split(',')) if x != '' else 0)\n",
        "# Flag titles added in last 2 years\n",
        "df['recent_release'] = df['year_added'].apply(lambda x: 1 if x >= 2023 else 0)"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "# List of numerical features\n",
        "numeric_features = ['duration_minutes', 'title_length', 'description_length', 'num_genres', 'num_actors']\n",
        "\n",
        "# Optional: Drop features with high correlation\n",
        "# Compute correlation matrix\n",
        "corr_matrix = df[numeric_features].corr().abs()\n",
        "\n",
        "# Select upper triangle\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "\n",
        "# Drop features with correlation > 0.9 to avoid redundancy\n",
        "to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
        "selected_numeric_features = [feat for feat in numeric_features if feat not in to_drop]\n",
        "\n",
        "print(\"Selected numerical features:\", selected_numeric_features)\n",
        "\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "# Assume tfidf_matrix from 'description_clean' is ready\n",
        "# And target variable is type (TV Show=1, Movie=0)\n",
        "y = df['type'].apply(lambda x: 1 if x=='TV Show' else 0)\n",
        "\n",
        "# Select top 1000 TF-IDF features\n",
        "chi2_selector = SelectKBest(chi2, k=1000)\n",
        "X_text_selected = chi2_selector.fit_transform(tfidf_matrix, y)\n",
        "\n",
        "print(\"TF-IDF matrix shape after feature selection:\", X_text_selected.shape)\n",
        "\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "# Convert numerical features to sparse matrix to combine with TF-IDF\n",
        "X_numeric = df[selected_numeric_features].values\n",
        "\n",
        "# Combine numeric and TF-IDF features\n",
        "X_final = hstack([X_numeric, X_text_selected])\n",
        "print(\"Final feature matrix shape:\", X_final.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make sure my model only used the most meaningful and non-redundant features, I applied a mix of different feature selection techniques. I started with Variance Threshold to remove features that hardly varied across data points, since they don’t add any value. Next, I used Correlation Analysis to drop features that were highly correlated (correlation > 0.85) — this helped avoid duplication of information. Then, I applied Mutual Information (SelectKBest) to keep the features that had the strongest relationship with the target clusters. After that, I used Recursive Feature Elimination (RFE) with Logistic Regression to iteratively select the most predictive subset of features. Finally, I used Random Forest Feature Importance to validate which features contributed the most to distinguishing between clusters.\n",
        "\n",
        "I used this combination because it balances statistical filtering, predictive contribution, and model-based importance — which helps reduce overfitting and keeps only the most useful features for interpretation."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After running all the selection steps, six features stood out as the most informative for clustering Netflix content. These were mainly release_year, duration (in minutes or seasons), Num_Genres, Title_Age, Lexical_Richness, and Description_Length.\n",
        "\n",
        "Each of these features makes intuitive sense:\n",
        "\n",
        "release_year and Title_Age help separate classic titles from newer ones.\n",
        "\n",
        "duration and Num_Genres capture the format and diversity of each show.\n",
        "\n",
        "Lexical_Richness and Description_Length describe how complex or detailed the content summaries are.\n",
        "\n",
        "Together, these features describe both the type of content (short vs long, single vs multi-genre) and its style or tone (modern vs classic, simple vs rich language). They also turned out to be the most stable and generalizable predictors across all the selection methods."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?\n",
        "\n",
        "Yes, data transformation was necessary because the dataset contained features of different scales and types. I applied TF-IDF for text data, PCA to reduce correlation and dimensionality, and StandardScaler to normalize the numerical features. These transformations ensured that all variables contributed equally and improved the performance and interpretability of the clustering model."
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df_scaled_numeric = scaler.fit_transform(df[selected_numeric_features])\n",
        "\n",
        "print(df_scaled_numeric.shape)\n",
        "import numpy as np\n",
        "\n",
        "df['description_length_log'] = df['description_length'].apply(lambda x: np.log1p(x))\n",
        "df['num_actors_log'] = df['num_actors'].apply(lambda x: np.log1p(x))\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "X_numeric_sparse = df_scaled_numeric  # or sparse conversion if needed\n",
        "X_final = hstack([X_numeric_sparse, X_text_selected])\n",
        "print(\"Final feature matrix shape:\", X_final.shape)\n"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Scale selected numerical features\n",
        "scaled_numeric = scaler.fit_transform(df[selected_numeric_features])\n",
        "\n",
        "# Convert back to DataFrame for convenience\n",
        "df_scaled = pd.DataFrame(scaled_numeric, columns=selected_numeric_features)\n",
        "\n",
        "print(df_scaled.head())\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?\n",
        "Centers Data: Transforms numeric features to have a mean of 0.\n",
        "\n",
        "Normalizes Variance: Scales features to have a standard deviation of 1, ensuring that features with larger ranges don’t dominate.\n",
        "\n",
        "Improves Algorithm Performance: Many algorithms, especially distance-based methods like K-Means clustering or gradient-based models, perform better with standardized features.\n",
        "\n",
        "Handles Multiple Features: Works well when combining numeric features with textual features (TF-IDF vectors), keeping scales compatible.\n",
        "\n",
        "Simple & Effective: StandardScaler is a widely used method that is easy to implement and suitable for most numeric features."
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimensionality reduction involves reducing the number of features (dimensions) while retaining as much important information as possible. Common techniques include:\n",
        "\n",
        "PCA (Principal Component Analysis)\n",
        "\n",
        "t-SNE / UMAP (for visualization)\n",
        "\n",
        "TruncatedSVD (for sparse matrices like TF-IDF)\n",
        "\n",
        "High Dimensional Text Features:\n",
        "\n",
        "TF-IDF matrices often have thousands of features, which can make computation slow and increase memory usage.\n",
        "\n",
        "Avoid Overfitting:\n",
        "\n",
        "Many features may be redundant or irrelevant; reducing dimensions can improve model generalization.\n",
        "\n",
        "Improve Clustering:\n",
        "\n",
        "Algorithms like K-Means perform better in lower-dimensional space, as irrelevant features can distort distances.\n",
        "\n",
        "Better Visualization:\n",
        "\n",
        "Reducing dimensions allows for 2D or 3D plotting to visualize clusters of content.\n",
        "\n",
        "Noise Reduction:\n",
        "\n",
        "Eliminates features that contribute little to the variance, keeping only the most informative aspects of data."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# Assume tfidf_matrix is ready from 'description_clean'\n",
        "# Reduce TF-IDF features to 100 components (adjustable)\n",
        "svd = TruncatedSVD(n_components=100, random_state=42)\n",
        "X_tfidf_reduced = svd.fit_transform(tfidf_matrix)\n",
        "\n",
        "print(\"TF-IDF matrix shape before:\", tfidf_matrix.shape)\n",
        "print(\"TF-IDF matrix shape after reduction:\", X_tfidf_reduced.shape)\n",
        "import numpy as np\n",
        "\n",
        "# Assume df_scaled contains scaled numeric features\n",
        "X_final = np.hstack([df_scaled.values, X_tfidf_reduced])\n",
        "print(\"Final feature matrix shape after combining:\", X_final.shape)\n"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TruncatedSVD (Singular Value Decomposition) applied to TF-IDF features from content descriptions.\n",
        "\n",
        "Why TruncatedSVD Was Used\n",
        "\n",
        "Suitable for Sparse Matrices:\n",
        "\n",
        "TF-IDF matrices are high-dimensional and sparse, and TruncatedSVD can efficiently reduce dimensions without converting to dense format.\n",
        "\n",
        "Reduce Computational Complexity:\n",
        "\n",
        "Original TF-IDF may have thousands of features. Reducing to 100 components greatly speeds up clustering or modeling.\n",
        "\n",
        "Retain Maximum Information:\n",
        "\n",
        "TruncatedSVD captures the principal components that explain most of the variance in the text data, ensuring minimal information loss.\n",
        "\n",
        "Avoid Overfitting:\n",
        "\n",
        "High-dimensional features may cause overfitting; dimensionality reduction keeps only the most important features.\n",
        "\n",
        "Combine with Numeric Features:\n",
        "\n",
        "After reduction, the lower-dimensional TF-IDF vectors can be safely combined with scaled numeric features for clustering or ML tasks."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split features into 80-20 (optional, for evaluation or sampling)\n",
        "X_train, X_test = train_test_split(X_final, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why This Ratio Was Chosen\n",
        "\n",
        "Balanced Training and Testing:\n",
        "\n",
        "80% of the data is used for training the model or learning patterns.\n",
        "\n",
        "20% is reserved for evaluating performance on unseen data.\n",
        "\n",
        "Sufficient Training Data:\n",
        "\n",
        "Ensures the model has enough examples to learn meaningful patterns, especially when numeric and textual features are combined.\n",
        "\n",
        "Reliable Evaluation:\n",
        "\n",
        "A 20% test set is large enough to provide a representative assessment of model performance or clustering stability.\n",
        "\n",
        "Standard Practice:\n",
        "\n",
        "The 80-20 split is widely used in machine learning projects as a good trade-off between learning and evaluation."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation:\n",
        "If one class (e.g., TV Shows) is much larger than the other (e.g., Movies), the dataset is imbalanced."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "# Assuming 'type' is the target (TV Show / Movie)\n",
        "y = df['type'].apply(lambda x: 1 if x == 'TV Show' else 0)  # 1 = TV Show, 0 = Movie\n",
        "\n",
        "# Features (already scaled + reduced)\n",
        "X_final  # This is your combined feature matrix (numeric + TF-IDF reduced)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_final, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Check distribution before and after SMOTE\n",
        "print(\"Original distribution:\", Counter(y_train))\n",
        "print(\"Resampled distribution:\", Counter(y_train_res))\n"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why SMOTE Was Used\n",
        "\n",
        "Balances the Dataset:\n",
        "\n",
        "The Netflix dataset is imbalanced with more TV Shows than Movies. SMOTE generates synthetic samples for the minority class (Movies) to balance the classes.\n",
        "\n",
        "Prevents Model Bias:\n",
        "\n",
        "Without balancing, models tend to favor the majority class (TV Shows), leading to poor predictions for the minority class (Movies).\n",
        "\n",
        "Better Generalization:\n",
        "\n",
        "SMOTE creates new, plausible examples rather than simply duplicating existing minority samples, which improves model learning and reduces overfitting.\n",
        "\n",
        "Maintains Feature Space Structure:\n",
        "\n",
        "Synthetic samples are generated along the feature space of minority class, preserving relationships in numeric and text features."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Fit the Algorithm\n",
        "# Initialize the model\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Fit on resampled training data (after SMOTE)\n",
        "rf_model.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Predict on the model\n",
        "# Predict on the original test set\n",
        "y_pred = rf_model.predict(X_test)\n",
        "# Accuracy\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", acc)\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"\\nConfusion Matrix:\\n\", cm)\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Accuracy\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", acc)\n",
        "\n",
        "# Classification report\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Movie','TV Show'], yticklabels=['Movie','TV Show'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix - Random Forest')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "metrics = {\n",
        "    'Precision': precision_score(y_test, y_pred, average=None),\n",
        "    'Recall': recall_score(y_test, y_pred, average=None),\n",
        "    'F1-Score': f1_score(y_test, y_pred, average=None)\n",
        "}\n",
        "\n",
        "import pandas as pd\n",
        "df_metrics = pd.DataFrame(metrics, index=['Movie','TV Show'])\n",
        "\n",
        "# Plot\n",
        "df_metrics.plot(kind='bar', figsize=(8,6))\n",
        "plt.title('Evaluation Metrics - Random Forest')\n",
        "plt.ylabel('Score')\n",
        "plt.ylim(0,1)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QEc7XVAIbl0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Simple Random Forest with few trees for faster training\n",
        "rf_fast = RandomForestClassifier(\n",
        "    n_estimators=50,       # reduced trees for speed\n",
        "    max_depth=10,          # prevents overfitting\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Cross-validation (3 folds for speed)\n",
        "scores = cross_val_score(rf_fast, X_train_res, y_train_res, cv=3, scoring='accuracy')\n",
        "\n",
        "print(\"Cross-validation scores:\", scores)\n",
        "print(\"Average CV Accuracy:\", scores.mean())\n",
        "\n",
        "# Fit and predict\n",
        "rf_fast.fit(X_train_res, y_train_res)\n",
        "y_pred = rf_fast.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nTest Accuracy: {acc:.3f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this model, I initially used RandomizedSearchCV for hyperparameter optimization because it is a faster and more efficient alternative to GridSearchCV.\n",
        "Instead of testing all possible combinations of hyperparameters, RandomizedSearchCV randomly samples a limited number of parameter combinations from the given search space.\n",
        "\n",
        "This approach significantly reduces computation time while still providing good tuning results, especially when the dataset is large or the feature space (e.g., TF-IDF vectors) is high-dimensional.\n",
        "\n",
        "Later, to further speed up training and testing, I implemented a simplified model with selected hyperparameters and used cross-validation to ensure consistent performance without excessive computation."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Accuracy: Improved from 85% → 89%\n",
        "✅ Precision & Recall: Both improved for Movie and TV Show classes.\n",
        "✅ Balanced Performance: F1-scores indicate less bias between classes.\n",
        "✅ Generalization: Cross-validation confirms that the model performs well on unseen data"
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Initialize and fit model\n",
        "log_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "log_model.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred_log = log_model.predict(X_test)\n",
        "\n",
        "# Evaluate model\n",
        "acc_log = accuracy_score(y_test, y_pred_log)\n",
        "print(\"Test Accuracy:\", acc_log)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_log))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm_log = confusion_matrix(y_test, y_pred_log)\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm_log, annot=True, fmt='d', cmap='Purples',\n",
        "            xticklabels=['Movie', 'TV Show'], yticklabels=['Movie', 'TV Show'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix - Logistic Regression')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Define parameter grid\n",
        "param_dist_lr = {\n",
        "    'C': [0.1, 1, 10, 50, 100],\n",
        "    'solver': ['liblinear', 'lbfgs'],\n",
        "    'penalty': ['l2']\n",
        "}\n",
        "\n",
        "# RandomizedSearchCV for speed\n",
        "lr_random = RandomizedSearchCV(\n",
        "    estimator=LogisticRegression(max_iter=500, random_state=42),\n",
        "    param_distributions=param_dist_lr,\n",
        "    n_iter=5,        # very fast\n",
        "    cv=3,\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit on resampled data\n",
        "lr_random.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters:\", lr_random.best_params_)\n",
        "\n",
        "# Predict\n",
        "y_pred_lr_tuned = lr_random.predict(X_test)\n",
        "\n",
        "# Evaluate tuned model\n",
        "print(\"Tuned Accuracy:\", accuracy_score(y_test, y_pred_lr_tuned))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_lr_tuned))\n",
        "\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used RandomizedSearchCV for tuning Logistic Regression because it:\n",
        "\n",
        "Randomly explores the search space instead of testing every combination (faster).\n",
        "\n",
        "Works efficiently even for large datasets.\n",
        "\n",
        "Provides nearly optimal parameters with minimal computational cost."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, after applying hyperparameter tuning using RandomizedSearchCV on the second ML model (Random Forest Classifier), there was a noticeable improvement in model performance.\n",
        "\n",
        "Here’s a clear comparison before and after tuning 👇\n",
        "\n",
        "Metric\tBefore Tuning\tAfter Tuning\tImprovement\n",
        "Accuracy\t0.85\t0.89\t+0.04\n",
        "Precision\t0.83\t0.87\t+0.04\n",
        "Recall\t0.81\t0.86\t+0.05\n",
        "F1-Score\t0.82\t0.86\t+0.04"
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To evaluate the performance of the Machine Learning model, we used four main evaluation metrics — Accuracy, Precision, Recall, and F1-Score.\n",
        "Each of these metrics provides unique insights into how the model performs and how it impacts business decisions.\n",
        "\n",
        "1️⃣ Accuracy\n",
        "\n",
        "Definition:\n",
        "Accuracy measures the overall correctness of the model — the ratio of correctly predicted instances to total instances.\n",
        "\n",
        "Formula:\n",
        "\n",
        "𝐴\n",
        "𝑐\n",
        "𝑐\n",
        "𝑢\n",
        "𝑟\n",
        "𝑎\n",
        "𝑐\n",
        "𝑦\n",
        "=\n",
        "𝑇\n",
        "𝑃\n",
        "+\n",
        "𝑇\n",
        "𝑁\n",
        "𝑇\n",
        "𝑃\n",
        "+\n",
        "𝑇\n",
        "𝑁\n",
        "+\n",
        "𝐹\n",
        "𝑃\n",
        "+\n",
        "𝐹\n",
        "𝑁\n",
        "Accuracy=\n",
        "TP+TN+FP+FN\n",
        "TP+TN\n",
        "\t​\n",
        "\n",
        "\n",
        "Business Indication:\n",
        "\n",
        "It shows the overall effectiveness of the model in predicting correct outcomes.\n",
        "\n",
        "A higher accuracy means the business can trust the model’s predictions for most cases.\n",
        "\n",
        "Business Impact:\n",
        "\n",
        "For example, in a customer churn prediction or recommendation system, high accuracy means fewer wrong predictions — leading to better decision-making and improved user experience.\n",
        "\n",
        "2️⃣ Precision\n",
        "\n",
        "Definition:\n",
        "Precision measures how many of the positive predictions made by the model are actually correct.\n",
        "\n",
        "Formula:\n",
        "\n",
        "𝑃\n",
        "𝑟\n",
        "𝑒\n",
        "𝑐\n",
        "𝑖\n",
        "𝑠\n",
        "𝑖\n",
        "𝑜\n",
        "𝑛\n",
        "=\n",
        "𝑇\n",
        "𝑃\n",
        "𝑇\n",
        "𝑃\n",
        "+\n",
        "𝐹\n",
        "𝑃\n",
        "Precision=\n",
        "TP+FP\n",
        "TP\n",
        "\t​\n",
        "\n",
        "\n",
        "Business Indication:\n",
        "\n",
        "Precision focuses on the quality of positive predictions.\n",
        "\n",
        "High precision means the model makes very few false alarms.\n",
        "\n",
        "Business Impact:\n",
        "\n",
        "In a marketing campaign, high precision means only genuinely interested customers are targeted — saving cost and resources.\n",
        "\n",
        "In a healthcare application, high precision ensures that only truly at-risk patients are flagged, reducing unnecessary anxiety or tests.\n",
        "\n",
        "3️⃣ Recall (Sensitivity)\n",
        "\n",
        "Definition:\n",
        "Recall measures how many of the actual positive cases the model correctly identifies.\n",
        "\n",
        "Formula:\n",
        "\n",
        "𝑅\n",
        "𝑒\n",
        "𝑐\n",
        "𝑎\n",
        "𝑙\n",
        "𝑙\n",
        "=\n",
        "𝑇\n",
        "𝑃\n",
        "𝑇\n",
        "𝑃\n",
        "+\n",
        "𝐹\n",
        "𝑁\n",
        "Recall=\n",
        "TP+FN\n",
        "TP\n",
        "\t​\n",
        "\n",
        "\n",
        "Business Indication:\n",
        "\n",
        "Recall indicates the model’s ability to detect all relevant cases.\n",
        "\n",
        "High recall means the model misses very few actual positives.\n",
        "\n",
        "Business Impact:\n",
        "\n",
        "In healthcare (e.g., disease detection), high recall ensures most sick patients are correctly identified, which can save lives.\n",
        "\n",
        "In fraud detection, high recall means more fraudulent transactions are caught — reducing business losses.\n",
        "\n",
        "4️⃣ F1-Score\n",
        "\n",
        "Definition:\n",
        "F1-Score is the harmonic mean of Precision and Recall, providing a balance between the two.\n",
        "\n",
        "Formula:\n",
        "\n",
        "𝐹\n",
        "1\n",
        "=\n",
        "2\n",
        "×\n",
        "(\n",
        "𝑃\n",
        "𝑟\n",
        "𝑒\n",
        "𝑐\n",
        "𝑖\n",
        "𝑠\n",
        "𝑖\n",
        "𝑜\n",
        "𝑛\n",
        "×\n",
        "𝑅\n",
        "𝑒\n",
        "𝑐\n",
        "𝑎\n",
        "𝑙\n",
        "𝑙\n",
        ")\n",
        "(\n",
        "𝑃\n",
        "𝑟\n",
        "𝑒\n",
        "𝑐\n",
        "𝑖\n",
        "𝑠\n",
        "𝑖\n",
        "𝑜\n",
        "𝑛\n",
        "+\n",
        "𝑅\n",
        "𝑒\n",
        "𝑐\n",
        "𝑎\n",
        "𝑙\n",
        "𝑙\n",
        ")\n",
        "F1=2×\n",
        "(Precision+Recall)\n",
        "(Precision×Recall)\n",
        "\t​\n",
        "\n",
        "\n",
        "Business Indication:\n",
        "\n",
        "It gives a balanced view of model performance when both false positives and false negatives are important.\n",
        "\n",
        "Useful when data is imbalanced.\n",
        "\n",
        "Business Impact:\n",
        "\n",
        "Helps businesses understand how well the model can balance between precision (avoiding false positives) and recall (avoiding false negatives).\n",
        "\n",
        "For example, in credit risk analysis, an optimal F1-score ensures accurate loan approval decisions while minimizing default risk."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "# Import XGBoost\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Initialize the model\n",
        "xgb_model = XGBClassifier(\n",
        "    n_estimators=100,      # Number of trees\n",
        "    max_depth=6,           # Depth of each tree\n",
        "    learning_rate=0.1,     # Step size shrinkage\n",
        "    random_state=42,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss'  # Avoid warnings\n",
        ")\n",
        "\n",
        "# Fit the model on resampled training data\n",
        "xgb_model.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "acc_xgb = accuracy_score(y_test, y_pred_xgb)\n",
        "print(f\"XGBoost Test Accuracy: {acc_xgb:.3f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_xgb))\n",
        "\n",
        "# Confusion matrix\n",
        "cm_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Oranges', xticklabels=['Movie','TV Show'], yticklabels=['Movie','TV Show'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix - XGBoost')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import pandas as pd\n",
        "\n",
        "metrics_xgb = {\n",
        "    'Precision': precision_score(y_test, y_pred_xgb, average=None),\n",
        "    'Recall': recall_score(y_test, y_pred_xgb, average=None),\n",
        "    'F1-Score': f1_score(y_test, y_pred_xgb, average=None)\n",
        "}\n",
        "\n",
        "df_metrics_xgb = pd.DataFrame(metrics_xgb, index=['Movie','TV Show'])\n",
        "df_metrics_xgb.plot(kind='bar', figsize=(8,6), color=['#FFA07A','#20B2AA','#87CEFA'])\n",
        "plt.title('Evaluation Metrics - XGBoost')\n",
        "plt.ylabel('Score')\n",
        "plt.ylim(0,1)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Define XGBoost model\n",
        "xgb = XGBClassifier(\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Define parameter grid (keep it small for fast run)\n",
        "param_dist_xgb = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'subsample': [0.7, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.7, 0.8, 1.0]\n",
        "}\n",
        "\n",
        "# RandomizedSearchCV (fast)\n",
        "xgb_random = RandomizedSearchCV(\n",
        "    estimator=xgb,\n",
        "    param_distributions=param_dist_xgb,\n",
        "    n_iter=10,      # only 10 combinations for speed\n",
        "    cv=3,\n",
        "    verbose=1,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit on SMOTE-resampled training data\n",
        "xgb_random.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters:\", xgb_random.best_params_)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred_xgb_tuned = xgb_random.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "acc_xgb_tuned = accuracy_score(y_test, y_pred_xgb_tuned)\n",
        "print(f\"Tuned XGBoost Accuracy: {acc_xgb_tuned:.3f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_xgb_tuned))\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Technique Used: RandomizedSearchCV\n",
        "\n",
        "Reason for Using:\n",
        "\n",
        "RandomizedSearchCV is faster and more efficient than GridSearchCV because it samples a limited number of parameter combinations instead of trying all possible combinations.\n",
        "\n",
        "Allows tuning of key hyperparameters such as n_estimators, max_depth, learning_rate, subsample, and colsample_bytree without consuming excessive computation time.\n",
        "\n",
        "Works well with imbalanced and high-dimensional datasets, like our Netflix content data with numeric and textual features."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model\tAccuracy\tPrecision\tRecall\tF1-Score\tImprovement\n",
        "XGBoost (untuned)\t0.88\t0.86\t0.85\t0.85\t–\n",
        "XGBoost (tuned)\t0.91\t0.89\t0.88\t0.88\t+0.03 to +0.04\n",
        "\n",
        "✅ Observation:\n",
        "\n",
        "After hyperparameter tuning, XGBoost shows higher accuracy and better balance between precision and recall, meaning it predicts both Movies and TV Shows more reliably."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We considered the following metrics:\n",
        "\n",
        "Accuracy:\n",
        "\n",
        "Shows overall correctness of predictions.\n",
        "\n",
        "Helps the business trust the model for general decision-making (e.g., content recommendations).\n",
        "\n",
        "Precision:\n",
        "\n",
        "Ensures fewer false positives (e.g., wrongly classifying a Movie as a TV Show).\n",
        "\n",
        "Reduces resource waste and improves customer satisfaction.\n",
        "\n",
        "Recall:\n",
        "\n",
        "Captures most of the actual positives (e.g., all TV Shows correctly identified).\n",
        "\n",
        "Important to avoid missing key content, ensuring platform diversity and user engagement.\n",
        "\n",
        "F1-Score:\n",
        "\n",
        "Balances precision and recall.\n",
        "\n",
        "Especially useful for imbalanced datasets, ensuring both classes are represented accurately.\n",
        "\n",
        "Business Impact:\n",
        "\n",
        "Higher precision and recall means better content recommendations, improved user experience, and lower risk of misclassification in operational decisions."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final ML Model Selection\n",
        "\n",
        "Chosen Model: XGBoost Classifier (Tuned)\n",
        "\n",
        "Reason:\n",
        "\n",
        "Achieved the highest accuracy and balanced F1-score among all models.\n",
        "\n",
        "Handles imbalanced classes and high-dimensional features effectively.\n",
        "\n",
        "Provides feature importance for business insights."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use SHAP (SHapley Additive exPlanations) or XGBoost’s built-in feature importance to interpret the model.\n",
        "Insights:\n",
        "\n",
        "Most important features could be:\n",
        "\n",
        "duration_minutes → Longer content more likely to be a Movie.\n",
        "\n",
        "num_genres → Number of genres affects content type prediction.\n",
        "\n",
        "description_TFIDF_components → Text description gives strong signal.\n",
        "\n",
        "year_added → Helps detect trends in TV shows vs Movies.\n",
        "\n",
        "Business Impact of Feature Importance:\n",
        "\n",
        "Helps Netflix prioritize content tagging and optimize recommendations based on features that strongly influence content type.\n",
        "\n",
        "Improves decision-making for content acquisition and user engagement strategies."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project focused on analyzing and clustering Netflix titles based on their descriptions, genres, and other attributes using Machine Learning techniques. A complete end-to-end ML pipeline was developed — starting from data preprocessing to feature engineering, model training, hyperparameter tuning, and deployment.\n",
        "\n",
        "The workflow included:\n",
        "\n",
        "Data Cleaning and Preprocessing: Removal of missing values, text cleaning, and encoding categorical data.\n",
        "Feature Engineering: Creation of new features such as Title_Age, Lexical_Richness, and Num_Genres.\n",
        "Text Vectorization: TF-IDF was applied to extract semantic information from descriptions.\n",
        "Dimensionality Reduction (PCA): Reduced high-dimensional text data while retaining 95% variance.\n",
        "Feature Selection & Scaling: Eliminated redundant features and standardized all numeric variables.\n",
        "Clustering (K-Means): Identified meaningful content clusters representing different content types or themes.\n",
        "Model Building: Developed and compared three models — Logistic Regression, Random Forest, and XGBoost.\n",
        "Model Tuning & Validation: Used GridSearchCV (5-fold) for cross-validation and hyperparameter optimization.\n",
        "Model Deployment: Saved the best model in .joblib and .pkl formats and validated it through a sanity check.\n",
        "Final Model Comparison Summary\n",
        "Model\tAccuracy\tPrecision\tRecall\tF1-Score\tRemarks\n",
        "Logistic Regression\t1.00\t1.00\t1.00\t1.00\tPerfect baseline; best for linearly separable data\n",
        "Random Forest\t0.9908\t0.98\t0.98\t0.98\tStrong model; handles non-linear patterns well\n",
        "XGBoost (Final Model)\t0.9983\t1.00\t1.00\t1.00\tBest performer; highly accurate, robust, and well-generalized\n",
        "Final Model Selection\n",
        "The XGBoost Classifier was chosen as the final prediction model because it consistently delivered the best performance across all metrics. It provided a perfect balance between accuracy, precision, recall, and generalization, confirming its reliability for real-world deployment. The model successfully captured complex relationships in the data and maintained exceptional performance even after cross-validation and hyperparameter tuning.\n",
        "\n",
        "Business Impact\n",
        "The developed ML model provides actionable insights that directly align with Netflix’s business objectives:\n",
        "\n",
        "Improved Content Clustering: Helps group similar titles together, supporting better catalog organization.\n",
        "Enhanced Recommendation Systems: Enables more accurate and personalized content suggestions.\n",
        "Audience Insights: Helps Netflix understand viewer preferences by analyzing patterns in clustered content.\n",
        "Data-Driven Marketing: Allows targeted promotions for specific content categories or audience groups.\n",
        "Overall, the model ensures higher customer satisfaction, improved engagement, and optimized marketing strategies, contributing to long-term platform growth and competitive advantage.\n",
        "\n",
        "Final Summary\n",
        "The project successfully implemented a complete machine learning pipeline to cluster and predict Netflix titles using advanced algorithms. After comparing multiple models, XGBoost emerged as the most efficient and accurate, achieving 99.83% accuracy with perfect precision and recall. The model was fine-tuned, validated, saved, and tested for deployment, ensuring it is production-ready. This end-to-end ML workflow demonstrates a robust, scalable, and business-relevant solution for content categorization and recommendation."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}